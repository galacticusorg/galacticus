!! Copyright 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018
!!    Andrew Benson <abenson@carnegiescience.edu>
!!
!! This file is part of Galacticus.
!!
!!    Galacticus is free software: you can redistribute it and/or modify
!!    it under the terms of the GNU General Public License as published by
!!    the Free Software Foundation, either version 3 of the License, or
!!    (at your option) any later version.
!!
!!    Galacticus is distributed in the hope that it will be useful,
!!    but WITHOUT ANY WARRANTY; without even the implied warranty of
!!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
!!    GNU General Public License for more details.
!!
!!    You should have received a copy of the GNU General Public License
!!    along with Galacticus.  If not, see <http://www.gnu.org/licenses/>.

!% Contains a module that implements useful MPI utilities.

module MPI_Utilities
  !% Implements useful MPI utilities.
#ifdef USEMPI
  use               :: MPI_F08
#endif
  !$ use            :: Locks
  use   , intrinsic :: ISO_C_Binding
  use               :: ISO_Varying_String
  use               :: Galacticus_Error
  private
  public :: mpiInitialize, mpiFinalize, mpiBarrier, mpiSelf, mpiCounter

  ! Define a type for interacting with MPI.
  type :: mpiObject
     private
     integer                                            :: rankValue     , countValue    , &
          &                                                nodeCountValue
     type   (varying_string)                            :: hostName
     integer                , allocatable, dimension(:) :: allRanks      , nodeAffinities
   contains
     !@ <objectMethods>
     !@   <object>mpiObject</object>
     !@   <objectMethod>
     !@     <method>isMaster</method>
     !@     <type>\logicalzero</type>
     !@     <arguments></arguments>
     !@     <description>Return true if this is the master process (i.e. rank-0 process).</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>isActive</method>
     !@     <type>\logicalzero</type>
     !@     <arguments></arguments>
     !@     <description>Return true if MPI is active.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>rank</method>
     !@     <type>\intzero</type>
     !@     <arguments></arguments>
     !@     <description>Return the rank of this process.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>count</method>
     !@     <type>\intzero</type>
     !@     <arguments></arguments>
     !@     <description>Return the total number of processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>rankLabel</method>
     !@     <type>\textcolor{red}{\textless type(varying\_string)\textgreater}</type>
     !@     <arguments></arguments>
     !@     <description>Return a label containing the rank of the process.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>nodeCount</method>
     !@     <type>\intzero</type>
     !@     <arguments></arguments>
     !@     <description>Return the number of nodes on which this MPI job is running.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>nodeAffinity</method>
     !@     <type>\intzero</type>
     !@     <arguments>\intzero\ [rank]\argin</arguments>
     !@     <description>Return the index of the node on which the MPI process of the given rank (or this process if no rank is given) is running.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>hostAffinity</method>
     !@     <type>\textcolor{red}{\textless type(varying\_string)\textgreater}</type>
     !@     <arguments></arguments>
     !@     <description>Return the name of the host on which this MPI process is running.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>requestData</method>
     !@     <type>\doubletwo|\inttwo|\logicaltwo</type>
     !@     <arguments>\intone requestFrom\argin, \doubleone|\intone|\logicalone array</arguments>
     !@     <description>Request the content of {\normalfont \ttfamily array} from each processes listed in {\normalfont \ttfamily requestFrom}.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>messageWaiting</method>
     !@     <type>\logicalzero</type>
     !@     <arguments>\intzero\ [from]\argin, \intzero\ [tag]\argin</arguments>
     !@     <description>Return true if a message is waiting, optionally from the specified process and with the specified tag.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>average</method>
     !@     <type>\doubleone</type>
     !@     <arguments>\doubleone array\argin</arguments>
     !@     <description>Return the average of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>median</method>
     !@     <type>\intone</type>
     !@     <arguments>\intone array\argin</arguments>
     !@     <description>Return the median of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>sum</method>
     !@     <type>\intzero|\intone</type>
     !@     <arguments>(\intzero|\intone|\doublezero|\doubleone|\doubletwo) array\argin</arguments>
     !@     <description>Return the sum of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>maxval</method>
     !@     <type>\doubleone</type>
     !@     <arguments>(\doublezero|\doubleone) array\argin</arguments>
     !@     <description>Return the maximum value of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>maxloc</method>
     !@     <type>\intone</type>
     !@     <arguments>\doubleone array\argin</arguments>
     !@     <description>Return the rank of the process with the maximum value of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>minval</method>
     !@     <type>\doublezero|\doubleone|\intzero|\intone</type>
     !@     <arguments>(\doublezero|\doubleone|\intzero|\intone) array\argin</arguments>
     !@     <description>Return the minimum value of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>any</method>
     !@     <type>\logicalzero</type>
     !@     <arguments>\logicalzero\ scalar\argin, \logicalzero\ [mask]\argin</arguments>
     !@     <description>Return true if any of {\normalfont \ttfamily scalar} is true over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>minloc</method>
     !@     <type>\intone</type>
     !@     <arguments>\doubleone array\argin</arguments>
     !@     <description>Return the rank of the process with the minimum value of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>gather</method>
     !@     <type>(\doubleone|\doubletwo|\doublethree|\inttwo)</type>
     !@     <arguments>(\doublezero|\doubleone|\doubletwo|\intone) array\argin</arguments>
     !@     <description>Gather arrays from all processes into an array of rank one higher.</description>
     !@   </objectMethod>
     !@ </objectMethods>
     procedure :: isMaster       => mpiIsMaster
     procedure :: isActive       => mpiIsActive
     procedure :: rank           => mpiGetRank
     procedure :: rankLabel      => mpiGetRankLabel
     procedure :: count          => mpiGetCount
     procedure :: nodeCount      => mpiGetNodeCount
     procedure :: nodeAffinity   => mpiGetNodeAffinity
     procedure :: hostAffinity   => mpiGetHostAffinity
     procedure ::                   mpiRequestData1D    , mpiRequestData2D       , &
          &                         mpiRequestDataInt1D , mpiRequestDataLogical1D
     generic   :: requestData    => mpiRequestData1D    , mpiRequestData2D       , &
          &                         mpiRequestDataInt1D , mpiRequestDataLogical1D
     procedure :: messageWaiting => mpiMessageWaiting
     procedure ::                   mpiAverageScalar    , mpiAverageArray
     generic   :: average        => mpiAverageScalar    , mpiAverageArray
     procedure ::                   mpiMedianArray
     generic   :: median         => mpiMedianArray
     procedure ::                   mpiSumScalarInt     , mpiSumArrayInt
     procedure ::                   mpiSumScalarDouble  , mpiSumArrayDouble      , &
          &                         mpiSumArrayTwoDouble
     generic   :: sum            => mpiSumScalarInt     , mpiSumArrayInt         , &
          &                         mpiSumScalarDouble  , mpiSumArrayDouble      , &
          &                         mpiSumArrayTwoDouble
     procedure ::                   mpiAnyLogicalScalar
     generic   :: any            => mpiAnyLogicalScalar
     procedure :: maxloc         => mpiMaxloc
     procedure ::                   mpiMaxvalScalar     , mpiMaxvalArray
     generic   :: maxval         => mpiMaxvalScalar     , mpiMaxvalArray
     procedure :: minloc         => mpiMinloc
     procedure ::                   mpiMinvalScalar     , mpiMinvalArray         , &
          &                         mpiMinValIntScalar  , mpiMinvalIntArray
     generic   :: minval         => mpiMinvalScalar     , mpiMinvalArray         , &
          &                         mpiMinValIntScalar  , mpiMinvalIntArray
     procedure ::                   mpiGather1D         , mpiGather2D            , &
          &                         mpiGatherScalar     , mpiGatherInt1D         , &
          &                         mpiGatherIntScalar
     generic   :: gather         => mpiGather1D         , mpiGather2D            , &
          &                         mpiGatherScalar     , mpiGatherInt1D         , &
          &                         mpiGatherIntScalar
  end type mpiObject
  
  ! Declare an object for interaction with MPI.
  type(mpiObject) :: mpiSelf

  ! Define an MPI counter type.
  type :: mpiCounter
     !% An MPI-global counter class. The counter can be incremented and will return a globally unique integer, beginning at 0.
#ifdef USEMPI
     type   (MPI_Win     )                            :: window
     type   (MPI_Datatype)                            :: typeClass
#endif
     integer(c_size_t    ), allocatable, dimension(:) :: counter
     !$ type(ompLock )                                :: ompLock_
   contains
     !@ <objectMethods>
     !@   <object>mpiCounter</object>
     !@   <objectMethod>
     !@     <method>increment</method>
     !@     <type>\textcolor{red}{\textless integer(c\_size\_t)\textgreater}</type>
     !@     <arguments></arguments>
     !@     <description>Increment the counter and return the new value.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>get</method>
     !@     <type>\textcolor{red}{\textless integer(c\_size\_t)\textgreater}</type>
     !@     <arguments></arguments>
     !@     <description>Get the current value of the counter.</description>
     !@   </objectMethod>
     !@ </objectMethods>
     final     ::              counterDestructor
     procedure :: increment => counterIncrement
     procedure :: get       => counterGet
  end type mpiCounter

  interface mpiCounter
     module procedure counterConstructor
  end interface mpiCounter
  
  ! Record of whether we're running under MPI or not.
  logical            :: mpiIsActiveValue=.false.

  ! Tags.
  integer, parameter :: tagRequestForData= 1, tagState=2
  integer, parameter :: nullRequester    =-1

contains

  subroutine mpiInitialize(mpiThreadingRequired)
    !% Initialize MPI.
#ifdef USEMPI
    use Memory_Management
    use Galacticus_Error
    use Hashes
#endif
    implicit none
    integer                              , optional    , intent(in   ) :: mpiThreadingRequired
#ifdef USEMPI
    integer                                                            :: i                   , iError             , &
         &                                                                mpiThreadingProvided, processorNameLength, &
         &                                                                iProcess
    character(len=MPI_MAX_PROCESSOR_NAME), dimension(1)                :: processorName
    character(len=MPI_MAX_PROCESSOR_NAME), dimension(:), allocatable   :: processorNames
    type     (integerScalarHash         )                              :: processCount
    !# <optionalArgument name="mpiThreadingRequired" defaultsTo="MPI_THREAD_FUNNELED" />

    if (mpiThreadingRequired_ == MPI_Thread_Single) then
       call MPI_Init              (                                           iError)
       if (iError               /= 0                    ) call Galacticus_Error_Report('failed to initialize MPI'                                        //{introspection:location})
    else
       call MPI_Init_Thread       (mpiThreadingRequired_,mpiThreadingProvided,iError)
       if (iError               /= 0                    ) call Galacticus_Error_Report('failed to initialize MPI'                                        //{introspection:location})
       if (mpiThreadingProvided <  mpiThreadingRequired_) call Galacticus_Error_Report('MPI library does not provide required level of threading support'//{introspection:location})
    end if
    call    MPI_Comm_Size         (MPI_Comm_World       ,mpiSelf%countValue  ,iError)
    if    (iError               /= 0                    ) call Galacticus_Error_Report('failed to determine MPI count'                                   //{introspection:location})
    call    MPI_Comm_Rank         (MPI_Comm_World       ,mpiSelf% rankValue  ,iError)
    if    (iError               /= 0                    ) call Galacticus_Error_Report('failed to determine MPI rank'                                    //{introspection:location})
    call    MPI_Get_Processor_Name(processorName(1)     ,processorNameLength ,iError)
    if    (iError               /= 0                    ) call Galacticus_Error_Report('failed to get MPI processor name'                                //{introspection:location})
    mpiSelf%hostName=trim(processorName(1))
    call mpiBarrier()
    ! Construct an array containing all ranks.
    call allocateArray(mpiSelf%allRanks,[mpiSelf%countValue],[0])
    forall(i=0:mpiSelf%countValue-1)
       mpiSelf%allRanks(i)=i
    end forall
    ! Get processor names from all processes.
    allocate(processorNames(0:mpiSelf%countValue-1))
    call MPI_AllGather(processorName,MPI_MAX_PROCESSOR_NAME,MPI_Character,processorNames,MPI_MAX_PROCESSOR_NAME,MPI_Character,MPI_Comm_World,iError)
    ! Count processes per node.
    call processCount%initialize()
    do iProcess=0,mpiSelf%countValue-1
       if (processCount%exists(trim(processorNames(iProcess)))) then
          call processCount%set(trim(processorNames(iProcess)),processCount%value(trim(processorNames(iProcess)))+1)
       else
          call processCount%set(trim(processorNames(iProcess)),1)
       end if
    end do
    mpiself%nodeCountValue=processCount%size()
    allocate(mpiSelf%nodeAffinities(0:mpiSelf%countValue-1))
    mpiSelf%nodeAffinities=-1
    do i=1,mpiSelf%nodeCountValue
       do iProcess=0,mpiSelf%countValue-1
          if (trim(processorNames(iProcess)) == processCount%key(i)) mpiSelf%nodeAffinities(iProcess)=i
       end do
    end do
    deallocate(processorNames)    
    ! Record that MPI is active.
    mpiIsActiveValue=.true.
#else
    !GCC$ attributes unused :: mpiThreadingRequired
#endif
    return
  end subroutine mpiInitialize
  
  subroutine mpiFinalize()
    !% Finalize MPI.
#ifdef USEMPI
    use Galacticus_Error
    implicit none
    integer :: iError
    
    call MPI_Finalize(iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to finalize MPI'//{introspection:location})
    ! Record that MPI is inactive.
    mpiIsActiveValue=.false.
#endif
    return
  end subroutine mpiFinalize
  
  subroutine mpiBarrier()
    !% Block until all MPI processes are synchronized.
#ifdef USEMPI
    implicit none
    integer :: iError
    
    call MPI_Barrier(MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI barrier failed'//{introspection:location})
#endif
    return
  end subroutine mpiBarrier
  
  logical function mpiIsActive(self)
    !% Return true if MPI is active.
    implicit none
    class(mpiObject), intent(in   ) :: self
    !GCC$ attributes unused :: self
 
    mpiIsActive=mpiIsActiveValue
    return
  end function mpiIsActive
  
  logical function mpiIsMaster(self)
    !% Return true if this is the master process.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
#ifdef USEMPI
    mpiIsMaster=(.not.self%isActive() .or. self%rank() == 0)
#else
    !GCC$ attributes unused :: self
    mpiIsMaster=.true.
#endif
    return
  end function mpiIsMaster
  
  integer function mpiGetRank(self)
    !% Return MPI rank.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
#ifdef USEMPI
    mpiGetRank=self%rankValue
#else
    !GCC$ attributes unused :: self
    mpiGetRank=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGetRank
  
  function mpiGetRankLabel(self)
    !% Return MPI rank label.
    use ISO_Varying_String
    implicit none
    type     (varying_string)                :: mpiGetRankLabel
    class    (mpiObject     ), intent(in   ) :: self
#ifdef USEMPI
    character(len=4         )                :: label
#endif

#ifdef USEMPI
    if (self%isActive()) then
       write (label,'(i4.4)') self%rankValue
       mpiGetRankLabel=label
    else
       mpiGetRankLabel=''
    end if
#else
    !GCC$ attributes unused :: self
    mpiGetRankLabel=''
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGetRankLabel
  
  integer function mpiGetCount(self)
    !% Return MPI count.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
#ifdef USEMPI
    mpiGetCount=self%countValue
#else
    !GCC$ attributes unused :: self
    mpiGetCount=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGetCount
  
  integer function mpiGetNodeCount(self)
    !% Return count of nodes used by MPI.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
#ifdef USEMPI
    mpiGetNodeCount=self%nodeCountValue
#else
    !GCC$ attributes unused :: self
    mpiGetNodeCount=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGetNodeCount
  
  integer function mpiGetNodeAffinity(self,rank)
    !% Return node affinity of given MPI process.
    implicit none
    class  (mpiObject), intent(in   )           :: self
    integer           , intent(in   ), optional :: rank
#ifdef USEMPI
    integer                                     :: rankActual
#endif
    
#ifdef USEMPI
    rankActual=self%rank()
    if (present(rank)) rankActual=rank
    mpiGetNodeAffinity=self%nodeAffinities(rankActual)
#else
    !GCC$ attributes unused :: self, rank
    mpiGetNodeAffinity=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGetNodeAffinity
  
  function mpiGetHostAffinity(self)
    !% Return host affinity of this MPI process.
    implicit none
    type (varying_string)                :: mpiGetHostAffinity
    class(mpiObject     ), intent(in   ) :: self
    
#ifdef USEMPI
    mpiGetHostAffinity=self%hostName
#else
    !GCC$ attributes unused :: self
    mpiGetHostAffinity=""
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGetHostAffinity
  
  logical function mpiMessageWaiting(self,from,tag)
    !% Return true if an MPI message (matching the optional {\normalfont \ttfamily from} and {\normalfont \ttfamily tag} if given) is waiting for receipt.
    implicit none
    class  (mpiObject ), intent(in   )           :: self
    integer            , intent(in   ), optional :: from         , tag
#ifdef USEMPI
    type   (MPI_Status)                          :: messageStatus
    integer                                      :: fromActual   , tagActual, iError
#endif

#ifdef USEMPI
    fromActual=MPI_Any_Source
    tagActual =MPI_Any_Tag
    if (present(from)) fromActual=from
    if (present(tag ) ) tagActual=tag
    call MPI_IProbe(fromActual,tagActual,MPI_Comm_World,mpiMessageWaiting,messageStatus,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to probe for waiting messages'//{introspection:location})
#else
    !GCC$ attributes unused :: self, from, tag
    mpiMessageWaiting=.false.
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMessageWaiting

  function mpiRequestData1D(self,requestFrom,array)
    !% Request and receive data from other MPI processes.
    implicit none
    class           (mpiObject), intent(in   )                                           :: self
    integer                    , intent(in   ), dimension(                            :) :: requestFrom
    double precision           , intent(in   ), dimension(          :                  ) :: array
    double precision                          , dimension(size(array),size(requestFrom)) :: mpiRequestData1D
#ifdef USEMPI
    double precision                          , dimension(size(array)                  ) :: receivedData
    integer                                   , dimension(                            1) :: requester       , requestedBy
    type            (MPI_Request)             , dimension(         0: self%countValue-1) :: requestFromID
    type            (MPI_Request), allocatable, dimension(                            :) :: requestID       , requestIDtemp
    type            (MPI_Status )                                                        :: messageStatus
    integer                                                                              :: i               , iError       , &
         &                                                                                  iRequest        , receivedFrom , &
         &                                                                                  j
#endif

#ifdef USEMPI
    ! Record our own rank as the requester.
    requester=self%rank()
    ! Send requests.
    call mpiBarrier()
    do i=0,self%count()-1
       if (any(requestFrom == i)) then
          call MPI_ISend(requester    ,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       else
          call MPI_ISend(nullRequester,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       end if
    end do
    call mpiBarrier()
    ! Check for waiting requests.
    allocate(requestID(size(requestFrom)))
    iRequest=0
    do i=0,self%count()-1
       ! Receive the request.
       call MPI_Recv(requestedBy,1,MPI_Integer,MPI_Any_Source,tagRequestForData,MPI_Comm_World,messageStatus,iError)
       ! Check for a non-null request.
       if (requestedBy(1) /= nullRequester) then
          ! Expand the requestID buffer as required.
          iRequest=iRequest+1
          if (iRequest > size(requestID)) then
             call Move_Alloc(requestID,requestIDtemp)
             allocate(requestID(2*iRequest))
             requestID(1:size(requestIDtemp))=requestIDtemp
             deallocate(requestIDtemp)
          end if
          ! Send our data in reply.
          call MPI_ISend(array,size(array),MPI_Double_Precision,requestedBy(1),tagState,MPI_Comm_World,requestID(iRequest),iError)
       end if
    end do
    call mpiBarrier()
    ! Wait until all of our sends have been received.
    do i=0,self%count()-1
       call MPI_Wait(requestFromID(i),messageStatus,iError)
    end do
    ! Receive data.
    do i=1,size(requestFrom)
       call MPI_Recv(receivedData,size(array),MPI_Double_Precision,MPI_Any_Source,tagState,MPI_Comm_World,messageStatus,iError)
       ! Find who sent this data and apply to the relevant part of the results array.
       receivedFrom=messageStatus%MPI_Source
       do j=1,size(requestFrom)
          if (requestFrom(j) == receivedFrom) mpiRequestData1D(:,j)=receivedData
       end do
    end do
    ! Wait until all of our sends have been received.
    do i=1,iRequest
       call MPI_Wait(requestID(i),messageStatus,iError)
    end do
    call mpiBarrier()
    ! Deallocate request ID workspace.
    deallocate(requestID)
#else
    !GCC$ attributes unused :: self, requestFrom, array
    mpiRequestData1D=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiRequestData1D

  function mpiRequestData2D(self,requestFrom,array)
    !% Request and receive data from other MPI processes.
    implicit none
    class           (mpiObject  ), intent(in   )                                                                   :: self
    integer                      , intent(in   ), dimension(                                                    :) :: requestFrom
    double precision             , intent(in   ), dimension(                :,                :                  ) :: array
    double precision                            , dimension(size(array,dim=1),size(array,dim=2),size(requestFrom)) :: mpiRequestData2D
#ifdef USEMPI
    double precision                            , dimension(size(array,dim=1),size(array,dim=2)                  ) :: receivedData
    integer                                     , dimension(                                                    1) :: requester       , requestedBy
    type            (MPI_Request)               , dimension(                                 0: self%countValue-1) :: requestFromID
    type            (MPI_Request), allocatable  , dimension(                                                    :) :: requestID       , requestIDtemp
    type            (MPI_Status )                                                                                  :: messageStatus 
    integer                                                                                                        :: i               , iError       , &
         &                                                                                                            iRequest        , j            , &
         &                                                                                                            receivedFrom
#endif
    
#ifdef USEMPI
    ! Record our own rank as the requester.
    requester=self%rank()
    ! Send requests.
    call mpiBarrier()
    do i=0,self%count()-1
       if (any(requestFrom == i)) then
          call MPI_ISend(requester    ,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       else
          call MPI_ISend(nullRequester,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       end if
    end do
    call mpiBarrier()
    ! Check for waiting requests.
    allocate(requestID(size(requestFrom)))
    iRequest=0
    do i=0,self%count()-1
       ! Receive the request.
       call MPI_Recv(requestedBy,1,MPI_Integer,MPI_Any_Source,tagRequestForData,MPI_Comm_World,messageStatus,iError)
       ! Check for a non-null request.
       if (requestedBy(1) /= nullRequester) then
          ! Expand the requestID buffer as required.
          iRequest=iRequest+1
          if (iRequest > size(requestID)) then
             call Move_Alloc(requestID,requestIDtemp)
             allocate(requestID(2*iRequest))
             requestID(1:size(requestIDtemp))=requestIDtemp
             deallocate(requestIDtemp)
          end if
          ! Send our data in reply.
          call MPI_ISend(array,product(shape(array)),MPI_Double_Precision,requestedBy(1),tagState,MPI_Comm_World,requestID(iRequest),iError)
       end if
    end do
    call mpiBarrier()
    ! Wait until all of our sends have been received.
    do i=0,self%count()-1
       call MPI_Wait(requestFromID(i),messageStatus,iError)
    end do
    ! Receive data.
    do i=1,size(requestFrom)
       call MPI_Recv(receivedData,product(shape(array)),MPI_Double_Precision,requestFrom(i),tagState,MPI_Comm_World,messageStatus,iError)
       ! Find who sent this data and apply to the relevant part of the results array.
       receivedFrom=messageStatus%MPI_Source
       do j=1,size(requestFrom)
          if (requestFrom(j) == receivedFrom) mpiRequestData2D(:,:,j)=receivedData
       end do
    end do
    ! Wait until all of our sends have been received.
    do i=1,iRequest
       call MPI_Wait(requestID(i),messageStatus,iError)
    end do
    call mpiBarrier()
    ! Deallocate request ID workspace.
    deallocate(requestID)
#else
    !GCC$ attributes unused :: self, requestFrom, array
    mpiRequestData2D=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiRequestData2D

  function mpiRequestDataInt1D(self,requestFrom,array)
    !% Request and receive data from other MPI processes.
    implicit none
    class  (mpiObject  ), intent(in   )                                           :: self
    integer             , intent(in   ), dimension(                            :) :: requestFrom
    integer             , intent(in   ), dimension(          :                  ) :: array
    integer                            , dimension(size(array),size(requestFrom)) :: mpiRequestDataInt1D
#ifdef USEMPI
    integer                            , dimension(size(array)                  ) :: receivedData
    integer                            , dimension(                            1) :: requester       , requestedBy
    type   (MPI_Request)               , dimension(         0: self%countValue-1) :: requestFromID
    type   (MPI_Request), allocatable  , dimension(                            :) :: requestID       , requestIDtemp
    type   (MPI_Status )                                                          :: messageStatus
    integer                                                                       :: i               , iError       , &
         &                                                                           iRequest        , j            , &
         &                                                                           receivedFrom
#endif
    
#ifdef USEMPI
    ! Record our own rank as the requester.
    requester=self%rank()
    ! Send requests.
    call mpiBarrier()
    do i=0,self%count()-1
       if (any(requestFrom == i)) then
          call MPI_ISend(requester    ,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       else
          call MPI_ISend(nullRequester,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       end if
    end do
    call mpiBarrier()
    ! Check for waiting requests.
    allocate(requestID(size(requestFrom)))
    iRequest=0
    do i=0,self%count()-1
       ! Receive the request.
       call MPI_Recv(requestedBy,1,MPI_Integer,MPI_Any_Source,tagRequestForData,MPI_Comm_World,messageStatus,iError)
       ! Check for a non-null request.
       if (requestedBy(1) /= nullRequester) then
          ! Expand the requestID buffer as required.
          iRequest=iRequest+1
          if (iRequest > size(requestID)) then
             call Move_Alloc(requestID,requestIDtemp)
             allocate(requestID(2*iRequest))
             requestID(1:size(requestIDtemp))=requestIDtemp
             deallocate(requestIDtemp)
          end if
          ! Send our data in reply.
          call MPI_ISend(array,size(array),MPI_Integer,requestedBy(1),tagState,MPI_Comm_World,requestID(iRequest),iError)
       end if
    end do
    call mpiBarrier()
    ! Wait until all of our sends have been received.
    do i=0,self%count()-1
       call MPI_Wait(requestFromID(i),messageStatus,iError)
    end do
    ! Receive data.
    do i=1,size(requestFrom)
       call MPI_Recv(receivedData,size(array),MPI_Integer,requestFrom(i),tagState,MPI_Comm_World,messageStatus,iError)
       ! Find who sent this data and apply to the relevant part of the results array.
       receivedFrom=messageStatus%MPI_Source
       do j=1,size(requestFrom)
          if (requestFrom(j) == receivedFrom) mpiRequestDataInt1D(:,j)=receivedData
       end do
    end do
    ! Wait until all of our sends have been received.
    do i=1,iRequest
       call MPI_Wait(requestID(i),messageStatus,iError)
    end do
    call mpiBarrier()
    ! Deallocate request ID workspace.
    deallocate(requestID)
#else
    !GCC$ attributes unused :: self, requestFrom, array
    mpiRequestDataInt1D=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiRequestDataInt1D

  function mpiRequestDataLogical1D(self,requestFrom,array)
    !% Request and receive data from other MPI processes.
    implicit none
    class  (mpiObject  ), intent(in   )                                           :: self
    integer             , intent(in   ), dimension(                            :) :: requestFrom
    logical             , intent(in   ), dimension(          :                  ) :: array
    logical                            , dimension(size(array),size(requestFrom)) :: mpiRequestDataLogical1D
#ifdef USEMPI
    logical                            , dimension(size(array)                  ) :: receivedData
    integer                            , dimension(                            1) :: requester       , requestedBy
    type   (MPI_Request)               , dimension(         0: self%countValue-1) :: requestFromID
    type   (MPI_Request), allocatable  , dimension(                            :) :: requestID       , requestIDtemp
    type   (MPI_Status )                                                          :: messageStatus
    integer                                                                       :: i               , iError       , &
         &                                                                           iRequest        , j            , &
         &                                                                           receivedFrom
#endif
    
#ifdef USEMPI
    ! Record our own rank as the requester.
    requester=self%rank()
    ! Send requests.
    call mpiBarrier()
    do i=0,self%count()-1
       if (any(requestFrom == i)) then
          call MPI_ISend(requester    ,1,MPI_Logical,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       else
          call MPI_ISend(nullRequester,1,MPI_Logical,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       end if
    end do
    call mpiBarrier()
    ! Check for waiting requests.
    allocate(requestID(size(requestFrom)))
    iRequest=0
    do i=0,self%count()-1
       ! Receive the request.
       call MPI_Recv(requestedBy,1,MPI_Logical,MPI_Any_Source,tagRequestForData,MPI_Comm_World,messageStatus,iError)
       ! Check for a non-null request.
       if (requestedBy(1) /= nullRequester) then
          ! Expand the requestID buffer as required.
          iRequest=iRequest+1
          if (iRequest > size(requestID)) then
             call Move_Alloc(requestID,requestIDtemp)
             allocate(requestID(2*iRequest))
             requestID(1:size(requestIDtemp))=requestIDtemp
             deallocate(requestIDtemp)
          end if
          ! Send our data in reply.
          call MPI_ISend(array,size(array),MPI_Logical,requestedBy(1),tagState,MPI_Comm_World,requestID(iRequest),iError)
       end if
    end do
    call mpiBarrier()
    ! Wait until all of our sends have been received.
    do i=0,self%count()-1
       call MPI_Wait(requestFromID(i),messageStatus,iError)
    end do
    ! Receive data.
    do i=1,size(requestFrom)
       call MPI_Recv(receivedData,size(array),MPI_Logical,requestFrom(i),tagState,MPI_Comm_World,messageStatus,iError)
       ! Find who sent this data and apply to the relevant part of the results array.
       receivedFrom=messageStatus%MPI_Source
       do j=1,size(requestFrom)
          if (requestFrom(j) == receivedFrom) mpiRequestDataLogical1D(:,j)=receivedData
       end do
    end do
    ! Wait until all of our sends have been received.
    do i=1,iRequest
       call MPI_Wait(requestID(i),messageStatus,iError)
    end do
    call mpiBarrier()
    ! Deallocate request ID workspace.
    deallocate(requestID)
#else
    !GCC$ attributes unused :: self, requestFrom, array
    mpiRequestDataLogical1D=.false.
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiRequestDataLogical1D

  function mpiSumArrayInt(self,array,mask)
    !% Sum an integer array over all processes, returning it to all processes.
    use Galacticus_Error
    implicit none
    class  (mpiObject), intent(in   )                                    :: self
    integer           , intent(in   ), dimension( :          )           :: array
    logical           , intent(in   ), dimension(0:          ), optional :: mask
    integer                          , dimension(size(array))            :: mpiSumArrayInt
#ifdef USEMPI
    integer                          , dimension(size(array))            :: maskedArray
    integer                                                              :: iError        , activeCount
#endif
    
#ifdef USEMPI
    ! Sum the array over all processes.
    maskedArray=array
    activeCount=self%count()
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=0
       activeCount=count(mask)
    end if
    call MPI_AllReduce(maskedArray,mpiSumArrayInt,size(array),MPI_Integer,MPI_Sum,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
#else
    !GCC$ attributes unused :: self, array, mask
    mpiSumArrayInt=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiSumArrayInt

  integer function mpiSumScalarInt(self,scalar,mask)
    !% Sum an integer scalar over all processes, returning it to all processes.
    implicit none
    class  (mpiObject), intent(in   )                         :: self
    integer           , intent(in   )                         :: scalar
    logical           , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    integer                          , dimension(1)           :: array
#endif

#ifdef USEMPI
    array=self%sum([scalar],mask)
    mpiSumScalarInt=array(1)
#else
    !GCC$ attributes unused :: self, scalar, mask
    mpiSumScalarInt=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiSumScalarInt

  function mpiSumArrayDouble(self,array,mask)
    !% Sum an integer array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    double precision           , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    double precision                          , dimension(size(array))            :: mpiSumArrayDouble
#ifdef USEMPI
    double precision                          , dimension(size(array))            :: maskedArray
    integer                                                                       :: iError           , activeCount
#endif
    
#ifdef USEMPI
    ! Sum the array over all processes.
    maskedArray=array
    activeCount=self%count()
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=0.0d0
       activeCount=count(mask)
    end if
    call MPI_AllReduce(maskedArray,mpiSumArrayDouble,size(array),MPI_Double_Precision,MPI_Sum,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
#else
    !GCC$ attributes unused :: self, array, mask
    mpiSumArrayDouble=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiSumArrayDouble
  
  function mpiSumArrayTwoDouble(self,array,mask)
    !% Sum an rank-2 double array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                                           :: self
    double precision           , intent(in   ), dimension( :               , :               )           :: array
    logical                    , intent(in   ), dimension(0:                                 ), optional :: mask
    double precision                          , dimension(size(array,dim=1),size(array,dim=2))           :: mpiSumArrayTwoDouble
#ifdef USEMPI
    double precision                          , dimension(size(array,dim=1),size(array,dim=2))            :: maskedArray
    integer                                                                                               :: iError           , activeCount
#endif
    
#ifdef USEMPI
    ! Sum the array over all processes.
    maskedArray=array
    activeCount=self%count()
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=0.0d0
       activeCount=count(mask)
    end if
    call MPI_AllReduce(maskedArray,mpiSumArrayTwoDouble,size(array),MPI_Double_Precision,MPI_Sum,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
#else
    !GCC$ attributes unused :: self, array, mask
    mpiSumArrayTwoDouble=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiSumArrayTwoDouble

  double precision function mpiSumScalarDouble(self,scalar,mask)
    !% Sum an integer scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    double precision           , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    double precision                          , dimension(1)           :: array
#endif
    
#ifdef USEMPI
    array=self%sum([scalar],mask)
    mpiSumScalarDouble=array(1)
#else
    !GCC$ attributes unused :: self, scalar, mask
    mpiSumScalarDouble=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiSumScalarDouble

  function mpiAverageArray(self,array,mask)
    !% Average an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    double precision           , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    double precision                          , dimension(size(array) )           :: mpiAverageArray
#ifdef USEMPI
    double precision                          , dimension(size(array) )           :: maskedArray
    integer                                                                       :: iError         , activeCount
#endif
    
#ifdef USEMPI
    ! Sum the array over all processes.
    maskedArray=array
    activeCount=self%count()
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=0.0d0
       activeCount=count(mask)
    end if
    call MPI_AllReduce(maskedArray,mpiAverageArray,size(array),MPI_Double_Precision,MPI_Sum,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
    ! Convert the sum into an average.
    mpiAverageArray=mpiAverageArray/dble(activeCount)
#else
    !GCC$ attributes unused :: self, array, mask
    mpiAverageArray=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiAverageArray

  function mpiMedianArray(self,array,mask)
    !% Find the median of an array over all processes, returning it to all processes.
#ifdef USEMPI
    use Sort
#endif
    implicit none
    class           (mpiObject), intent(in   )                                                   :: self
    integer                    , intent(in   ), dimension(:                          )           :: array
    logical                    , intent(in   ), dimension(:                          ), optional :: mask
    integer                                   , dimension(size(array)                )           :: mpiMedianArray
#ifdef USEMPI
    integer                                   , dimension(size(array),self%countValue)           :: allArray
    integer                                   , dimension(1:2                        )           :: indexMedian
    integer                                                                                      :: i             , activeCount
#endif
    
#ifdef USEMPI
    ! Get count of active process.
    if (present(mask)) then
       activeCount=self%countValue-count(mask)
    else
       activeCount=self%countValue
    end if
    ! Find the indices corresponding to the median.
    if (mod(activeCount,2) == 1) then
       indexMedian=               activeCount/2+1
    else
       indexMedian=[activeCount/2,activeCount/2+1]
    end if
    ! Gather the array from all processes.
    allArray=self%gather(array)
    ! Iterate over array index.
    do i=1,size(array)
       ! Set masked values to huge.
       if (present(mask)) then
          where (mask)
             allArray(i,:)=huge(1)
          end where
       end if
       ! Sort over processes.
       call Sort_Do(allArray(i,:))
       ! Compute the median.
       mpiMedianArray(i)=(allArray(i,indexMedian(1))+allArray(i,indexMedian(2)))/2
    end do
#else
    !GCC$ attributes unused :: self, array, mask
    mpiMedianArray=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMedianArray

  double precision function mpiAverageScalar(self,scalar,mask)
    !% Find the maximum values of a scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    double precision           , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    double precision                          , dimension(1)           :: array
#endif
    
#ifdef USEMPI
    array=self%average([scalar],mask)
    mpiAverageScalar=array(1)
#else
    !GCC$ attributes unused :: self, scalar, mask
    mpiAverageScalar=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiAverageScalar

  function mpiMaxvalArray(self,array,mask)
    !% Find the maximum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    double precision           , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    double precision                          , dimension(size(array) )           :: mpiMaxvalArray
#ifdef USEMPI
    double precision                          , dimension(size(array) )           :: maskedArray
    integer                                                                       :: iError
#endif

#ifdef USEMPI
    ! Find the maximum over all processes.
    maskedArray=array
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=-HUGE(1.0d0)
    end if
    call MPI_AllReduce(maskedArray,mpiMaxvalArray,size(array),MPI_Double_Precision,MPI_Max,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
#else
    !GCC$ attributes unused :: self, array, mask
    mpiMaxvalArray=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMaxvalArray

  double precision function mpiMaxvalScalar(self,scalar,mask)
    !% Find the maximum values of a scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    double precision           , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    double precision                          , dimension(1)           :: array
#endif
    
#ifdef USEMPI
    array=self%maxval([scalar],mask)
    mpiMaxvalScalar=array(1)
#else
    !GCC$ attributes unused :: self, scalar, mask
    mpiMaxvalScalar=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMaxvalScalar

  function mpiMaxloc(self,array,mask)
    !% Find the rank of the process having maximum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                      :: self
    double precision           , intent(in   ), dimension( :            )           :: array
    logical                    , intent(in   ), dimension(0:            ), optional :: mask
    integer                                   , dimension(   size(array))           :: mpiMaxloc
#ifdef USEMPI
    double precision                          , dimension(2 ,size(array))           :: arrayIn  , arrayOut
    integer                                                                         :: iError
#endif
    
#ifdef USEMPI
    ! Find the maximum over all processes.
    arrayIn(1,:)=array
    if (present(mask)) then
       if (.not.mask(self%rank())) arrayIn(1,:)=-HUGE(1.0d0)
    end if
    arrayIn(2,:)=self%rank()
    call MPI_AllReduce(arrayIn,arrayOut,size(array),MPI_2Double_Precision,MPI_MaxLoc,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
    mpiMaxloc=int(arrayOut(2,:))
#else
    !GCC$ attributes unused :: self, array, mask
    mpiMaxloc=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMaxloc

  function mpiMinvalArray(self,array,mask)
    !% Find the minimum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    double precision           , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    double precision                          , dimension(size(array) )           :: mpiMinvalArray
#ifdef USEMPI
    double precision                          , dimension(size(array) )           :: maskedArray
    integer                                                                       :: iError
#endif
    
#ifdef USEMPI
   ! Find the minimum over all processes.
    maskedArray=array
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=-HUGE(1.0d0)
    end if
    call MPI_AllReduce(maskedArray,mpiMinvalArray,size(array),MPI_Double_Precision,MPI_Min,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
#else
    !GCC$ attributes unused :: self, array, mask
    mpiMinvalArray=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMinvalArray

  function mpiMinvalIntArray(self,array,mask)
    !% Find the minimum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    integer                    , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    integer                                   , dimension(size(array) )           :: mpiMinvalIntArray
#ifdef USEMPI
    integer                                   , dimension(size(array) )           :: maskedArray
    integer                                                                       :: iError
#endif
    
#ifdef USEMPI
   ! Find the minimum over all processes.
    maskedArray=array
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=-huge(1)
    end if
    call MPI_AllReduce(maskedArray,mpiMinvalIntArray,size(array),MPI_Integer,MPI_Min,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
#else
    !GCC$ attributes unused :: self, array, mask
    mpiMinvalIntArray=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMinvalIntArray
  
  double precision function mpiMinvalScalar(self,scalar,mask)
    !% Find the minimum values of a scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    double precision           , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    double precision                          , dimension(1)           :: array
#endif
    
#ifdef USEMPI
    array=self%minval([scalar],mask)
    mpiMinvalScalar=array(1)
#else
    !GCC$ attributes unused :: self, scalar, mask
    mpiMinvalScalar=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMinvalScalar

  integer function mpiMinvalIntScalar(self,scalar,mask)
    !% Find the minimum values of a scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    integer                    , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    integer                                   , dimension(1)           :: array
#endif
    
#ifdef USEMPI
    array=self%minval([scalar],mask)
    mpiMinvalIntScalar=array(1)
#else
    !GCC$ attributes unused :: self, scalar, mask
    mpiMinvalIntScalar=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMinvalIntScalar

  function mpiMinloc(self,array,mask)
    !% Find the rank of the process having minimum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                      :: self
    double precision           , intent(in   ), dimension( :            )           :: array
    logical                    , intent(in   ), dimension(0:            ), optional :: mask
    integer                                   , dimension(   size(array))           :: mpiMinloc
#ifdef USEMPI
    double precision                          , dimension(2 ,size(array))           :: arrayIn  , arrayOut
    integer                                                                         :: iError
#endif
    
#ifdef USEMPI
    ! Find the minimum over all processes.
    arrayIn(1,:)=array
    if (present(mask)) then
       if (.not.mask(self%rank())) arrayIn(1,:)=-HUGE(1.0d0)
    end if
    arrayIn(2,:)=self%rank()
    call MPI_AllReduce(arrayIn,arrayOut,size(array),MPI_2Double_Precision,MPI_MinLoc,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('MPI all reduce failed'//{introspection:location})
    mpiMinloc=int(arrayOut(2,:))
#else
    !GCC$ attributes unused :: self, array, mask
    mpiMinloc=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiMinloc

  logical function mpiAnyLogicalScalar(self,boolean,mask)
    !% Return true if any of the given booleans is true over all processes.
#ifdef USEMPI
    use Galacticus_Error
#endif
    implicit none
    class  (mpiObject), intent(in   )                         :: self
    logical           , intent(in   )                         :: boolean
    logical           , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    integer                                                   :: iError
    logical                          , dimension(1)           :: array
#endif

#ifdef USEMPI
    array=boolean
    if (present(mask)) then
       if (.not.mask(self%rank())) array=.false.
    end if
    call MPI_AllReduce(array,mpiAnyLogicalScalar,size(array),MPI_Logical,MPI_LOr,MPI_Comm_World,iError)
#else
    !GCC$ attributes unused :: self, boolean, mask
    mpiAnyLogicalScalar=.false.
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiAnyLogicalScalar

  function mpiGatherScalar(self,scalar)
    !% Gather a scalar from all processes, returning it as a 1-D array.
    implicit none
    class           (mpiObject), intent(in   )                :: self
    double precision           , intent(in   )                :: scalar
    double precision           , dimension(  self%countValue) :: mpiGatherScalar
#ifdef USEMPI
    double precision           , dimension(1,self%countValue) :: array
#endif
    
#ifdef USEMPI
    array=self%requestData(self%allRanks,[scalar])
    mpiGatherScalar=array(1,:)
#else
    !GCC$ attributes unused :: self, scalar
    mpiGatherScalar=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGatherScalar
  
  function mpiGather1D(self,array)
    !% Gather a 1-D array from all processes, returning it as a 2-D array.
    implicit none
    class           (mpiObject), intent(in   )                                         :: self
    double precision           , intent(in   ), dimension(          :                ) :: array
    double precision           ,                dimension(size(array),self%countValue) :: mpiGather1D

#ifdef USEMPI
    mpiGather1D=self%requestData(self%allRanks,array)
#else
    !GCC$ attributes unused :: self, array
    mpiGather1D=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGather1D

  function mpiGather2D(self,array)
    !% Gather a 1-D array from all processes, returning it as a 2-D array.
    implicit none
    class           (mpiObject), intent(in   )                                                                 :: self
    double precision           , intent(in   ), dimension(                :,                :                ) :: array
    double precision           ,                dimension(size(array,dim=1),size(array,dim=2),self%countValue) :: mpiGather2D

#ifdef USEMPI
    mpiGather2D=self%requestData(self%allRanks,array)
#else
    !GCC$ attributes unused :: self, array
    mpiGather2D=0.0d0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGather2D

  function mpiGatherIntScalar(self,scalar)
    !% Gather an integre scalar from all processes, returning it as a 1-D array.
    implicit none
    class           (mpiObject), intent(in   )                :: self
    integer                    , intent(in   )                :: scalar
    integer                    , dimension(  self%countValue) :: mpiGatherIntScalar
#ifdef USEMPI
    integer                    , dimension(1,self%countValue) :: array
#endif
    
#ifdef USEMPI
    array=self%requestData(self%allRanks,[scalar])
    mpiGatherIntScalar=array(1,:)
#else
    !GCC$ attributes unused :: self, scalar
    mpiGatherIntScalar=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGatherIntScalar
  
  function mpiGatherInt1D(self,array)
    !% Gather an integer 1-D array from all processes, returning it as a 2-D array.
    implicit none
    class  (mpiObject), intent(in   )                                         :: self
    integer           , intent(in   ), dimension(          :                ) :: array
    integer           ,                dimension(size(array),self%countValue) :: mpiGatherInt1D

#ifdef USEMPI
    mpiGatherInt1D=self%requestData(self%allRanks,array)
#else
    !GCC$ attributes unused :: self, array
    mpiGatherInt1D=0
    call Galacticus_Error_Report('code was not compiled for MPI'//{introspection:location})
#endif
    return
  end function mpiGatherInt1D

  function counterConstructor() result(self)
    !% Constructor for MPI counter class.
    use, intrinsic :: ISO_C_Binding
    use               Galacticus_Error
    implicit none
    type   (mpiCounter      ) :: self
#ifdef USEMPI
    integer                   :: mpiSize, iError

    call MPI_SizeOf(0_c_size_t,mpiSize,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to get type size'//{introspection:location})
    call MPI_Type_Match_Size(MPI_TypeClass_Integer,mpiSize,self%typeClass,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to get type'     //{introspection:location})
    if (mpiSelf%rank() == 0) then
       ! The rank-0 process allocates space for the counter and creates its window.
       allocate(self%counter(1))
       self%counter=0
       call MPI_Win_Create(self%counter,int(mpiSize,kind=MPI_Address_Kind),mpiSize,MPI_Info_Null,MPI_Comm_World,self%window,iError)
       if (iError /= 0) call Galacticus_Error_Report('failed to create RMA window'//{introspection:location})
    else
       ! Other processes create a zero-size window.
       call MPI_Win_Create(C_Null_Ptr  ,               0_MPI_Address_Kind,mpiSize,MPI_Info_Null,MPI_Comm_World,self%window,iError)
       if (iError /= 0) call Galacticus_Error_Report('failed to create RMA window'//{introspection:location})
    end if
#else
    allocate(self%counter(1))
    self%counter=0
#endif
    !$ self%ompLock_=ompLock()
    return
  end function counterConstructor

  subroutine counterDestructor(self)
    !% Destructor for the MPI counter class.
    implicit none
    type   (mpiCounter), intent(inout) :: self
#ifdef USEMPI
    integer                            :: iError

    call MPI_Win_Free(self%window,iError)
#else
    !GCC$ attributes unused :: self
#endif
    return
  end subroutine counterDestructor

  function counterIncrement(self)
    !% Increment an MPI counter.
    use Galacticus_Error
    implicit none
    integer(c_size_t  )                :: counterIncrement
    class  (mpiCounter), intent(inout) :: self
#ifdef USEMPI
    integer(c_size_t  ), dimension(1)  :: counterIn       , counterOut
    integer                            :: iError
    
    counterIn=1
    !$ call self%ompLock_%  set()
    call MPI_Win_Lock(MPI_Lock_Exclusive,0,0,self%window,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to lock RMA window'          //{introspection:location})
    call MPI_Get_Accumulate(counterIn,1,self%typeClass,counterOut,1,self%typeClass,0,0_MPI_Address_Kind,1,self%typeClass,MPI_Sum,self%window,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to accumulate to MPI counter'//{introspection:location})
    call MPI_Win_Unlock(0,self%window,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to unlock RMA window'        //{introspection:location})
    !$ call self%ompLock_%unset()
    counterIncrement=counterOut(1)
#else
    !$ call self%ompLock_%  set()
    counterIncrement=self%counter(1)
    self%counter(1)=self%counter(1)+1_c_size_t
    !$ call self%ompLock_%unset()
#endif
    return
  end function counterIncrement

 function counterGet(self)
    !% Return the current value of an MPI counter.
    use Galacticus_Error
    implicit none
    integer(c_size_t  )                :: counterGet
    class  (mpiCounter), intent(inout) :: self
#ifdef USEMPI
    integer(c_size_t  ), dimension(1)  :: counterOut
    integer                            :: iError
    
    !$ call self%ompLock_%  set()
    call MPI_Win_Lock(MPI_Lock_Exclusive,0,0,self%window,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to lock RMA window'           //{introspection:location})
    call MPI_Get(counterOut,1,self%typeClass,0,0_MPI_Address_Kind,1,self%typeClass,self%window,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to get value from MPI counter'//{introspection:location})
    call MPI_Win_Unlock(0,self%window,iError)
    if (iError /= 0) call Galacticus_Error_Report('failed to unlock RMA window'         //{introspection:location})
    !$ call self%ompLock_%unset()
    counterGet=counterOut(1)-1
#else
    !$ call self%ompLock_%  set()
    counterGet=self%counter(1)-1_c_size_t
    !$ call self%ompLock_%unset()
#endif
    return
  end function counterGet

end module MPI_Utilities
