!! Copyright 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016
!!    Andrew Benson <abenson@obs.carnegiescience.edu>
!!
!! This file is part of Galacticus.
!!
!!    Galacticus is free software: you can redistribute it and/or modify
!!    it under the terms of the GNU General Public License as published by
!!    the Free Software Foundation, either version 3 of the License, or
!!    (at your option) any later version.
!!
!!    Galacticus is distributed in the hope that it will be useful,
!!    but WITHOUT ANY WARRANTY; without even the implied warranty of
!!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
!!    GNU General Public License for more details.
!!
!!    You should have received a copy of the GNU General Public License
!!    along with Galacticus.  If not, see <http://www.gnu.org/licenses/>.

!% Contains a module that implements useful MPI utilities.

module MPI_Utilities
  !% Implements useful MPI utilities.
#ifdef USEMPI
  use MPI
#endif
  use ISO_Varying_String
  use Galacticus_Error
  private
  public :: mpiInitialize, mpiFinalize, mpiBarrier, mpiSelf

  ! Define a type for interacting with MPI.
  type :: mpiObject
     private
     integer                                            :: rankValue, countValue    , nodeCountValue
     type   (varying_string)                            :: hostName
     integer                , allocatable, dimension(:) :: allRanks , nodeAffinities
   contains
     !@ <objectMethods>
     !@   <object>mpiObject</object>
     !@   <objectMethod>
     !@     <method>isMaster</method>
     !@     <type>\logicalzero</type>
     !@     <arguments></arguments>
     !@     <description>Return true if this is the master process (i.e. rank-0 process).</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>isActive</method>
     !@     <type>\logicalzero</type>
     !@     <arguments></arguments>
     !@     <description>Return true if MPI is active.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>rank</method>
     !@     <type>\intzero</type>
     !@     <arguments></arguments>
     !@     <description>Return the rank of this process.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>count</method>
     !@     <type>\intzero</type>
     !@     <arguments></arguments>
     !@     <description>Return the total number of processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>rankLabel</method>
     !@     <type>\textcolor{red}{\textless type(varying\_string)\textgreater}</type>
     !@     <arguments></arguments>
     !@     <description>Return a label containing the rank of the process.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>nodeCount</method>
     !@     <type>\intzero</type>
     !@     <arguments></arguments>
     !@     <description>Return the number of nodes on which this MPI job is running.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>nodeAffinity</method>
     !@     <type>\intzero</type>
     !@     <arguments>\intzero\ [rank]\argin</arguments>
     !@     <description>Return the index of the node on which the MPI process of the given rank (or this process if no rank is given) is running.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>hostAffinity</method>
     !@     <type>\textcolor{red}{\textless type(varying\_string)\textgreater}</type>
     !@     <arguments></arguments>
     !@     <description>Return the name of the host on which this MPI process is running.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>requestData</method>
     !@     <type>\doubletwo|\inttwo</type>
     !@     <arguments>\intone requestFrom\argin, \doubleone|\intone array</arguments>
     !@     <description>Request the content of {\normalfont \ttfamily array} from each processes listed in {\normalfont \ttfamily requestFrom}.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>messageWaiting</method>
     !@     <type>\logicalzero</type>
     !@     <arguments>\intzero\ [from]\argin, \intzero\ [tag]\argin</arguments>
     !@     <description>Return true if a message is waiting, optionally from the specified process and with the specified tag.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>average</method>
     !@     <type>\doubleone</type>
     !@     <arguments>\doubleone array\argin</arguments>
     !@     <description>Return the average of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>median</method>
     !@     <type>\intone</type>
     !@     <arguments>\intone array\argin</arguments>
     !@     <description>Return the median of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>sum</method>
     !@     <type>\intzero|\intone</type>
     !@     <arguments>(\intzero|\intone) array\argin</arguments>
     !@     <description>Return the sum of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>maxval</method>
     !@     <type>\doubleone</type>
     !@     <arguments>(\doublezero|\doubleone) array\argin</arguments>
     !@     <description>Return the maximum value of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>maxloc</method>
     !@     <type>\intone</type>
     !@     <arguments>\doubleone array\argin</arguments>
     !@     <description>Return the rank of the process with the maximum value of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>minval</method>
     !@     <type>\doubleone</type>
     !@     <arguments>(\doublezero|\doubleone) array\argin</arguments>
     !@     <description>Return the minimum value of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>minloc</method>
     !@     <type>\intone</type>
     !@     <arguments>\doubleone array\argin</arguments>
     !@     <description>Return the rank of the process with the minimum value of {\normalfont \ttfamily array} over all processes.</description>
     !@   </objectMethod>
     !@   <objectMethod>
     !@     <method>gather</method>
     !@     <type>(\doubleone|\doubletwo|\doublethree|\inttwo)</type>
     !@     <arguments>(\doublezero|\doubleone|\doubletwo|\intone) array\argin</arguments>
     !@     <description>Gather arrays from all processes into an array of rank one higher.</description>
     !@   </objectMethod>
     !@ </objectMethods>
     procedure :: isMaster       => mpiIsMaster
     procedure :: isActive       => mpiIsActive
     procedure :: rank           => mpiGetRank
     procedure :: rankLabel      => mpiGetRankLabel
     procedure :: count          => mpiGetCount
     procedure :: nodeCount      => mpiGetNodeCount
     procedure :: nodeAffinity   => mpiGetNodeAffinity
     procedure :: hostAffinity   => mpiGetHostAffinity
     procedure ::                   mpiRequestData1D   , mpiRequestData2D, &
          &                         mpiRequestDataInt1D
     generic   :: requestData    => mpiRequestData1D   , mpiRequestData2D, &
          &                         mpiRequestDataInt1D
     procedure :: messageWaiting => mpiMessageWaiting
     procedure ::                   mpiAverageScalar   , mpiAverageArray
     generic   :: average        => mpiAverageScalar   , mpiAverageArray
     procedure ::                   mpiMedianArray
     generic   :: median         => mpiMedianArray
     procedure ::                   mpiSumScalarInt    , mpiSumArrayInt
     procedure ::                   mpiSumScalarDouble , mpiSumArrayDouble
     generic   :: sum            => mpiSumScalarInt    , mpiSumArrayInt   , &
          &                         mpiSumScalarDouble , mpiSumArrayDouble
     procedure :: maxloc         => mpiMaxloc
     procedure ::                   mpiMaxvalScalar    , mpiMaxvalArray
     generic   :: maxval         => mpiMaxvalScalar    , mpiMaxvalArray
     procedure :: minloc         => mpiMinloc
     procedure ::                   mpiMinvalScalar    , mpiMinvalArray
     generic   :: minval         => mpiMinvalScalar    , mpiMinvalArray
     procedure ::                   mpiGather1D        , mpiGather2D      , &
          &                         mpiGatherScalar    , mpiGatherInt1D
     generic   :: gather         => mpiGather1D        , mpiGather2D      , &
          &                         mpiGatherScalar    , mpiGatherInt1D
  end type mpiObject

  ! Declare an object for interaction with MPI.
  type(mpiObject) :: mpiSelf

  ! Record of whether we're running under MPI or not.
  logical         :: mpiIsActiveValue=.false.

  ! Tags.
  integer, parameter :: tagRequestForData= 1, tagState=2
  integer, parameter :: nullRequester    =-1

contains

  subroutine mpiInitialize()
    !% Initialize MPI.
#ifdef USEMPI
    use Memory_Management
    use Galacticus_Error
    use Hashes
    implicit none
    integer                                                          :: i, iError, MPI_Threading_Provided, processorNameLength, iProcess
    character(len=MPI_MAX_PROCESSOR_NAME), dimension(1)              :: processorName
    character(len=MPI_MAX_PROCESSOR_NAME), dimension(:), allocatable :: processorNames
    type     (integerScalarHash         )                            :: processCount
    
    call MPI_Init_Thread(MPI_THREAD_FUNNELED,MPI_Threading_Provided,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiInitialize','failed to initialize MPI'     )
    call MPI_Comm_Size(MPI_Comm_World,mpiSelf%countValue,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiInitialize','failed to determine MPI count')
    call MPI_Comm_Rank(MPI_Comm_World,mpiSelf% rankValue,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiInitialize','failed to determine MPI rank' )
    call MPI_Get_Processor_Name(processorName(1),processorNameLength,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiInitialize','failed to get MPI processor name' )
    mpiSelf%hostName=trim(processorName(1))
    call mpiBarrier()
    ! Construct an array containing all ranks.
    call Alloc_Array(mpiSelf%allRanks,[mpiSelf%countValue],[0])
    forall(i=0:mpiSelf%countValue-1)
       mpiSelf%allRanks(i)=i
    end forall
    ! Get processor names from all processes.
    allocate(processorNames(0:mpiSelf%countValue-1))
    call MPI_AllGather(processorName,MPI_MAX_PROCESSOR_NAME,MPI_Character,processorNames,MPI_MAX_PROCESSOR_NAME,MPI_Character,MPI_Comm_World,iError)
    ! Count processes per node.
    call processCount%initialize()
    do iProcess=0,mpiSelf%countValue-1
       if (processCount%exists(trim(processorNames(iProcess)))) then
          call processCount%set(trim(processorNames(iProcess)),processCount%value(trim(processorNames(iProcess)))+1)
       else
          call processCount%set(trim(processorNames(iProcess)),1)
       end if
    end do
    mpiself%nodeCountValue=processCount%size()
    allocate(mpiSelf%nodeAffinities(0:mpiSelf%countValue-1))
    mpiSelf%nodeAffinities=-1
    do i=1,mpiSelf%nodeCountValue
       do iProcess=0,mpiSelf%countValue-1
          if (trim(processorNames(iProcess)) == processCount%key(i)) mpiSelf%nodeAffinities(iProcess)=i
       end do
    end do
    deallocate(processorNames)    
    ! Record that MPI is active.
    mpiIsActiveValue=.true.
#endif
    return
  end subroutine mpiInitialize
  
  subroutine mpiFinalize()
    !% Finalize MPI.
#ifdef USEMPI
    use Galacticus_Error
    implicit none
    integer :: iError
    
    call MPI_Finalize(iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiFinalize','failed to finalize MPI')
    ! Record that MPI is inactive.
    mpiIsActiveValue=.false.
#endif
    return
  end subroutine mpiFinalize
  
  subroutine mpiBarrier()
    !% Block until all MPI processes are synchronized.
#ifdef USEMPI
    implicit none
    integer :: iError
    
    call MPI_Barrier(MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiBarrier','MPI barrier failed')
#endif
    return
  end subroutine mpiBarrier
  
  logical function mpiIsActive(self)
    !% Return true if MPI is active.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
    mpiIsActive=mpiIsActiveValue
    return
  end function mpiIsActive
  
  logical function mpiIsMaster(self)
    !% Return true if this is the master process.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
#ifdef USEMPI
    mpiIsMaster=(.not.self%isActive() .or. self%rank() == 0)
#else
    mpiIsMaster=.true.
#endif
    return
  end function mpiIsMaster
  
  integer function mpiGetRank(self)
    !% Return MPI rank.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
#ifdef USEMPI
    mpiGetRank=self%rankValue
#else
    mpiGetRank=0
    call Galacticus_Error_Report('mpiGetRank','code was not compiled for MPI')
#endif
    return
  end function mpiGetRank
  
  function mpiGetRankLabel(self)
    !% Return MPI rank label.
    use ISO_Varying_String
    implicit none
    type     (varying_string)                :: mpiGetRankLabel
    class    (mpiObject     ), intent(in   ) :: self
#ifdef USEMPI
    character(len=4         )                :: label

    if (self%isActive()) then
       write (label,'(i4.4)') self%rankValue
       mpiGetRankLabel=label
    else
       mpiGetRankLabel=''
    end if
#else
    mpiGetRankLabel=''
    call Galacticus_Error_Report('mpiGetRankLabel','code was not compiled for MPI')
#endif
    return
  end function mpiGetRankLabel
  
  integer function mpiGetCount(self)
    !% Return MPI count.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
#ifdef USEMPI
    mpiGetCount=self%countValue
#else
    mpiGetCount=0
    call Galacticus_Error_Report('mpiGetCount','code was not compiled for MPI')
#endif
    return
  end function mpiGetCount
  
  integer function mpiGetNodeCount(self)
    !% Return count of nodes used by MPI.
    implicit none
    class(mpiObject), intent(in   ) :: self
    
#ifdef USEMPI
    mpiGetNodeCount=self%nodeCountValue
#else
    mpiGetNodeCount=0
    call Galacticus_Error_Report('mpiGetNodeCount','code was not compiled for MPI')
#endif
    return
  end function mpiGetNodeCount
  
  integer function mpiGetNodeAffinity(self,rank)
    !% Return node affinity of given MPI process.
    implicit none
    class  (mpiObject), intent(in   )           :: self
    integer           , intent(in   ), optional :: rank
#ifdef USEMPI
    integer                                     :: rankActual
    
    rankActual=self%rank()
    if (present(rank)) rankActual=rank
    mpiGetNodeAffinity=self%nodeAffinities(rankActual)
#else
    mpiGetNodeAffinity=0
    call Galacticus_Error_Report('mpiGetNodeAffinity','code was not compiled for MPI')
#endif
    return
  end function mpiGetNodeAffinity
  
  function mpiGetHostAffinity(self)
    !% Return host affinity of this MPI process.
    implicit none
    type (varying_string)                :: mpiGetHostAffinity
    class(mpiObject     ), intent(in   ) :: self
    
#ifdef USEMPI
    mpiGetHostAffinity=self%hostName
#else
    mpiGetHostAffinity=""
    call Galacticus_Error_Report('mpiGetHostAffinity','code was not compiled for MPI')
#endif
    return
  end function mpiGetHostAffinity
  
  logical function mpiMessageWaiting(self,from,tag)
    !% Return true if an MPI message (matching the optional {\normalfont \ttfamily from} and {\normalfont \ttfamily tag} if given) is waiting for receipt.
    implicit none
    class  (mpiObject), intent(in   )                        :: self
    integer           , intent(in   )             , optional :: from         , tag
#ifdef USEMPI
    integer           , dimension(MPI_Status_Size)           :: messageStatus
    integer                                                  :: fromActual   , tagActual, iError
   
    fromActual=MPI_Any_Source
    tagActual =MPI_Any_Tag
    if (present(from)) fromActual=from
    if (present(tag ) ) tagActual=tag
    call MPI_IProbe(fromActual,tagActual,MPI_Comm_World,mpiMessageWaiting,messageStatus,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiMessageWaiting','failed to probe for waiting messages')
#else
    mpiMessageWaiting=.false.
    call Galacticus_Error_Report('mpiMessageWaiting','code was not compiled for MPI')
#endif
    return
  end function mpiMessageWaiting

  function mpiRequestData1D(self,requestFrom,array)
    !% Request and receive data from other MPI processes.
    implicit none
    class           (mpiObject), intent(in   )                                           :: self
    integer                    , intent(in   ), dimension(                            :) :: requestFrom
    double precision           , intent(in   ), dimension(          :                  ) :: array
    double precision                          , dimension(size(array),size(requestFrom)) :: mpiRequestData1D
#ifdef USEMPI
    double precision                          , dimension(size(array)                  ) :: receivedData
    integer                                   , dimension(                            1) :: requester       , requestedBy
    integer                                   , dimension(         0: self%countValue-1) :: requestFromID
    integer                    , allocatable  , dimension(                            :) :: requestID       , requestIDtemp
    integer                                   , dimension(              MPI_Status_Size) :: messageStatus
    integer                                                                              :: i               , iError       , &
         &                                                                                  iRequest        , receivedFrom , &
         &                                                                                  j
    
    ! Record our own rank as the requester.
    requester=self%rank()
    ! Send requests.
    call mpiBarrier()
    do i=0,self%count()-1
       if (any(requestFrom == i)) then
          call MPI_ISend(requester    ,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       else
          call MPI_ISend(nullRequester,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       end if
    end do
    call mpiBarrier()
    ! Check for waiting requests.
    allocate(requestID(size(requestFrom)))
    iRequest=0
    do i=0,self%count()-1
       ! Receive the request.
       call MPI_Recv(requestedBy,1,MPI_Integer,MPI_Any_Source,tagRequestForData,MPI_Comm_World,messageStatus,iError)
       ! Check for a non-null request.
       if (requestedBy(1) /= nullRequester) then
          ! Expand the requestID buffer as required.
          iRequest=iRequest+1
          if (iRequest > size(requestID)) then
             call Move_Alloc(requestID,requestIDtemp)
             allocate(requestID(2*iRequest))
             requestID(1:size(requestIDtemp))=requestIDtemp
             deallocate(requestIDtemp)
          end if
          ! Send our data in reply.
          call MPI_ISend(array,size(array),MPI_Double_Precision,requestedBy(1),tagState,MPI_Comm_World,requestID(iRequest),iError)
       end if
    end do
    call mpiBarrier()
    ! Wait until all of our sends have been received.
    do i=0,self%count()-1
       call MPI_Wait(requestFromID(i),messageStatus,iError)
    end do
    ! Receive data.
    do i=1,size(requestFrom)
       call MPI_Recv(receivedData,size(array),MPI_Double_Precision,MPI_Any_Source,tagState,MPI_Comm_World,messageStatus,iError)
       ! Find who sent this data and apply to the relevant part of the results array.
       receivedFrom=messageStatus(MPI_SOURCE)
       do j=1,size(requestFrom)
          if (requestFrom(j) == receivedFrom) mpiRequestData1D(:,j)=receivedData
       end do
    end do
    ! Wait until all of our sends have been received.
    do i=1,iRequest
       call MPI_Wait(requestID(i),messageStatus,iError)
    end do
    call mpiBarrier()
    ! Deallocate request ID workspace.
    deallocate(requestID)
#else
    mpiRequestData1D=0.0d0
    call Galacticus_Error_Report('mpiRequestData1D','code was not compiled for MPI')
#endif
    return
  end function mpiRequestData1D

  function mpiRequestData2D(self,requestFrom,array)
    !% Request and receive data from other MPI processes.
    implicit none
    class           (mpiObject), intent(in   )                                                                   :: self
    integer                    , intent(in   ), dimension(                                                    :) :: requestFrom
    double precision           , intent(in   ), dimension(                :,                :                  ) :: array
    double precision                          , dimension(size(array,dim=1),size(array,dim=2),size(requestFrom)) :: mpiRequestData2D
#ifdef USEMPI
    double precision                          , dimension(size(array,dim=1),size(array,dim=2)                  ) :: receivedData
    integer                                   , dimension(                                                    1) :: requester       , requestedBy
    integer                                   , dimension(                                 0: self%countValue-1) :: requestFromID
    integer                    , allocatable  , dimension(                                                    :) :: requestID       , requestIDtemp
    integer                                   , dimension(                                      MPI_Status_Size) :: messageStatus
    integer                                                                                                      :: i               , iError       , &
         &                                                                                                          iRequest        , j            , &
         &                                                                                                          receivedFrom

    ! Record our own rank as the requester.
    requester=self%rank()
    ! Send requests.
    call mpiBarrier()
    do i=0,self%count()-1
       if (any(requestFrom == i)) then
          call MPI_ISend(requester    ,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       else
          call MPI_ISend(nullRequester,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       end if
    end do
    call mpiBarrier()
    ! Check for waiting requests.
    allocate(requestID(size(requestFrom)))
    iRequest=0
    do i=0,self%count()-1
       ! Receive the request.
       call MPI_Recv(requestedBy,1,MPI_Integer,MPI_Any_Source,tagRequestForData,MPI_Comm_World,messageStatus,iError)
       ! Check for a non-null request.
       if (requestedBy(1) /= nullRequester) then
          ! Expand the requestID buffer as required.
          iRequest=iRequest+1
          if (iRequest > size(requestID)) then
             call Move_Alloc(requestID,requestIDtemp)
             allocate(requestID(2*iRequest))
             requestID(1:size(requestIDtemp))=requestIDtemp
             deallocate(requestIDtemp)
          end if
          ! Send our data in reply.
          call MPI_ISend(array,product(shape(array)),MPI_Double_Precision,requestedBy(1),tagState,MPI_Comm_World,requestID(iRequest),iError)
       end if
    end do
    call mpiBarrier()
    ! Wait until all of our sends have been received.
    do i=0,self%count()-1
       call MPI_Wait(requestFromID(i),messageStatus,iError)
    end do
    ! Receive data.
    do i=1,size(requestFrom)
       call MPI_Recv(receivedData,product(shape(array)),MPI_Double_Precision,requestFrom(i),tagState,MPI_Comm_World,messageStatus,iError)
       ! Find who sent this data and apply to the relevant part of the results array.
       receivedFrom=messageStatus(MPI_SOURCE)
       do j=1,size(requestFrom)
          if (requestFrom(j) == receivedFrom) mpiRequestData2D(:,:,j)=receivedData
       end do
    end do
    ! Wait until all of our sends have been received.
    do i=1,iRequest
       call MPI_Wait(requestID(i),messageStatus,iError)
    end do
    call mpiBarrier()
    ! Deallocate request ID workspace.
    deallocate(requestID)
#else
    mpiRequestData2D=0.0d0
    call Galacticus_Error_Report('mpiRequestData2D','code was not compiled for MPI')
#endif
    return
  end function mpiRequestData2D

  function mpiRequestDataInt1D(self,requestFrom,array)
    !% Request and receive data from other MPI processes.
    implicit none
    class  (mpiObject), intent(in   )                                           :: self
    integer           , intent(in   ), dimension(                            :) :: requestFrom
    integer           , intent(in   ), dimension(          :                  ) :: array
    integer                          , dimension(size(array),size(requestFrom)) :: mpiRequestDataInt1D
#ifdef USEMPI
    integer                          , dimension(size(array)                  ) :: receivedData
    integer                          , dimension(                            1) :: requester       , requestedBy
    integer                          , dimension(         0: self%countValue-1) :: requestFromID
    integer           , allocatable  , dimension(                            :) :: requestID       , requestIDtemp
    integer                          , dimension(              MPI_Status_Size) :: messageStatus
    integer                                                                     :: i               , iError       , &
         &                                                                         iRequest        , j            , &
         &                                                                         receivedFrom
    
    ! Record our own rank as the requester.
    requester=self%rank()
    ! Send requests.
    call mpiBarrier()
    do i=0,self%count()-1
       if (any(requestFrom == i)) then
          call MPI_ISend(requester    ,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       else
          call MPI_ISend(nullRequester,1,MPI_Integer,i,tagRequestForData,MPI_Comm_World,requestFromID(i),iError)
       end if
    end do
    call mpiBarrier()
    ! Check for waiting requests.
    allocate(requestID(size(requestFrom)))
    iRequest=0
    do i=0,self%count()-1
       ! Receive the request.
       call MPI_Recv(requestedBy,1,MPI_Integer,MPI_Any_Source,tagRequestForData,MPI_Comm_World,messageStatus,iError)
       ! Check for a non-null request.
       if (requestedBy(1) /= nullRequester) then
          ! Expand the requestID buffer as required.
          iRequest=iRequest+1
          if (iRequest > size(requestID)) then
             call Move_Alloc(requestID,requestIDtemp)
             allocate(requestID(2*iRequest))
             requestID(1:size(requestIDtemp))=requestIDtemp
             deallocate(requestIDtemp)
          end if
          ! Send our data in reply.
          call MPI_ISend(array,size(array),MPI_Integer,requestedBy(1),tagState,MPI_Comm_World,requestID(iRequest),iError)
       end if
    end do
    call mpiBarrier()
    ! Wait until all of our sends have been received.
    do i=0,self%count()-1
       call MPI_Wait(requestFromID(i),messageStatus,iError)
    end do
    ! Receive data.
    do i=1,size(requestFrom)
       call MPI_Recv(receivedData,size(array),MPI_Integer,requestFrom(i),tagState,MPI_Comm_World,messageStatus,iError)
       ! Find who sent this data and apply to the relevant part of the results array.
       receivedFrom=messageStatus(MPI_SOURCE)
       do j=1,size(requestFrom)
          if (requestFrom(j) == receivedFrom) mpiRequestDataInt1D(:,j)=receivedData
       end do
    end do
    ! Wait until all of our sends have been received.
    do i=1,iRequest
       call MPI_Wait(requestID(i),messageStatus,iError)
    end do
    call mpiBarrier()
    ! Deallocate request ID workspace.
    deallocate(requestID)
#else
    mpiRequestDataInt1D=0
    call Galacticus_Error_Report('mpiRequestDataInt1D','code was not compiled for MPI')
#endif
    return
  end function mpiRequestDataInt1D

  function mpiSumArrayInt(self,array,mask)
    !% Sum an integer array over all processes, returning it to all processes.
    use Galacticus_Error
    implicit none
    class  (mpiObject), intent(in   )                                    :: self
    integer           , intent(in   ), dimension( :          )           :: array
    logical           , intent(in   ), dimension(0:          ), optional :: mask
    integer                          , dimension(size(array))            :: mpiSumArrayInt
#ifdef USEMPI
    integer                          , dimension(size(array))            :: maskedArray
    integer                                                              :: iError        , activeCount

    ! Sum the array over all processes.
    maskedArray=array
    activeCount=self%count()
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=0
       activeCount=count(mask)
    end if
    call MPI_AllReduce(maskedArray,mpiSumArrayInt,size(array),MPI_Integer,MPI_Sum,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiSumArrayInt','MPI all reduce failed')
#else
    mpiSumArrayInt=0
    call Galacticus_Error_Report('mpiSumArrayInt','code was not compiled for MPI')
#endif
    return
  end function mpiSumArrayInt

  integer function mpiSumScalarInt(self,scalar,mask)
    !% Sum an integer scalar over all processes, returning it to all processes.
    implicit none
    class  (mpiObject), intent(in   )                         :: self
    integer           , intent(in   )                         :: scalar
    logical           , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    integer                          , dimension(1)           :: array

    array=self%sum([scalar],mask)
    mpiSumScalarInt=array(1)
#else
    mpiSumScalarInt=0
    call Galacticus_Error_Report('mpisumScalarInt','code was not compiled for MPI')
#endif
    return
  end function mpiSumScalarInt

  function mpiSumArrayDouble(self,array,mask)
    !% Sum an integer array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    double precision           , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    double precision                          , dimension(size(array))            :: mpiSumArrayDouble
#ifdef USEMPI
    double precision                          , dimension(size(array))            :: maskedArray
    integer                                                                       :: iError           , activeCount

    ! Sum the array over all processes.
    maskedArray=array
    activeCount=self%count()
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=0
       activeCount=count(mask)
    end if
    call MPI_AllReduce(maskedArray,mpiSumArrayDouble,size(array),MPI_Double_Precision,MPI_Sum,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiSumArrayDouble','MPI all reduce failed')
#else
    mpiSumArrayDouble=0.0d0
    call Galacticus_Error_Report('mpiSumArrayDouble','code was not compiled for MPI')
#endif
    return
  end function mpiSumArrayDouble

  double precision function mpiSumScalarDouble(self,scalar,mask)
    !% Sum an integer scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    double precision           , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    double precision                          , dimension(1)           :: array

    array=self%sum([scalar],mask)
    mpiSumScalarDouble=array(1)
#else
    mpiSumScalarDouble=0.0d0
    call Galacticus_Error_Report('mpisumScalarDouble','code was not compiled for MPI')
#endif
    return
  end function mpiSumScalarDouble

  function mpiAverageArray(self,array,mask)
    !% Average an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    double precision           , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    double precision                          , dimension(size(array) )           :: mpiAverageArray
#ifdef USEMPI
    double precision                          , dimension(size(array) )           :: maskedArray
    integer                                                                       :: iError         , activeCount

    ! Sum the array over all processes.
    maskedArray=array
    activeCount=self%count()
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=0.0d0
       activeCount=count(mask)
    end if
    call MPI_AllReduce(maskedArray,mpiAverageArray,size(array),MPI_Double_Precision,MPI_Sum,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiAverageArray','MPI all reduce failed')
    ! Convert the sum into an average.
    mpiAverageArray=mpiAverageArray/dble(activeCount)
#else
    mpiAverageArray=0.0d0
    call Galacticus_Error_Report('mpiAverageArray','code was not compiled for MPI')
#endif
    return
  end function mpiAverageArray

  function mpiMedianArray(self,array,mask)
    !% Find the median of an array over all processes, returning it to all processes.
#ifdef USEMPI
    use Sort
#endif
    implicit none
    class           (mpiObject), intent(in   )                                                   :: self
    integer                    , intent(in   ), dimension(:                          )           :: array
    logical                    , intent(in   ), dimension(:                          ), optional :: mask
    integer                                   , dimension(size(array)                )           :: mpiMedianArray
#ifdef USEMPI
    integer                                   , dimension(size(array),self%countValue)           :: allArray
    integer                                   , dimension(1:2                        )           :: indexMedian
    integer                                                                                      :: i             , activeCount

    ! Get count of active process.
    if (present(mask)) then
       activeCount=self%countValue-count(mask)
    else
       activeCount=self%countValue
    end if
    ! Find the indices corresponding to the median.
    if (mod(activeCount,2) == 1) then
       indexMedian=               activeCount/2+1
    else
       indexMedian=[activeCount/2,activeCount/2+1]
    end if
    ! Gather the array from all processes.
    allArray=self%gather(array)
    ! Iterate over array index.
    do i=1,size(array)
       ! Set masked values to huge.
       if (present(mask)) then
          where (mask)
             allArray(i,:)=huge(1)
          end where
       end if
       ! Sort over processes.
       call Sort_Do(allArray(i,:))
       ! Compute the median.
       mpiMedianArray(i)=(allArray(i,indexMedian(1))+allArray(i,indexMedian(2)))/2
    end do
#else
    mpiMedianArray=0.0d0
    call Galacticus_Error_Report('mpiMedianArray','code was not compiled for MPI')
#endif
    return
  end function mpiMedianArray

  double precision function mpiAverageScalar(self,scalar,mask)
    !% Find the maximum values of a scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    double precision           , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    double precision                          , dimension(1)           :: array

    array=self%average([scalar],mask)
    mpiAverageScalar=array(1)
#else
    mpiAverageScalar=0.0d0
    call Galacticus_Error_Report('mpiAverageScalar','code was not compiled for MPI')
#endif
    return
  end function mpiAverageScalar

  function mpiMaxvalArray(self,array,mask)
    !% Find the maximum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    double precision           , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    double precision                          , dimension(size(array) )           :: mpiMaxvalArray
#ifdef USEMPI
    double precision                          , dimension(size(array) )           :: maskedArray
    integer                                                                       :: iError

    ! Find the maximum over all processes.
    maskedArray=array
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=-HUGE(1.0d0)
    end if
    call MPI_AllReduce(maskedArray,mpiMaxvalArray,size(array),MPI_Double_Precision,MPI_Max,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiMaxvalArray','MPI all reduce failed')
#else
    mpiMaxvalArray=0.0d0
    call Galacticus_Error_Report('mpiMaxvalArray','code was not compiled for MPI')
#endif
    return
  end function mpiMaxvalArray

  double precision function mpiMaxvalScalar(self,scalar,mask)
    !% Find the maximum values of a scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    double precision           , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    double precision                          , dimension(1)           :: array

    array=self%maxval([scalar],mask)
    mpiMaxvalScalar=array(1)
#else
    mpiMaxvalScalar=0.0d0
    call Galacticus_Error_Report('mpiMaxvalScalar','code was not compiled for MPI')
#endif
    return
  end function mpiMaxvalScalar

  function mpiMaxloc(self,array,mask)
    !% Find the rank of the process having maximum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                      :: self
    double precision           , intent(in   ), dimension( :            )           :: array
    logical                    , intent(in   ), dimension(0:            ), optional :: mask
    integer                                   , dimension(   size(array))           :: mpiMaxloc
#ifdef USEMPI
    double precision                          , dimension(2 ,size(array))           :: arrayIn  , arrayOut
    integer                                                                         :: iError

    ! Find the maximum over all processes.
    arrayIn(1,:)=array
    if (present(mask)) then
       if (.not.mask(self%rank())) arrayIn(1,:)=-HUGE(1.0d0)
    end if
    arrayIn(2,:)=self%rank()
    call MPI_AllReduce(arrayIn,arrayOut,size(array),MPI_2Double_Precision,MPI_MaxLoc,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiMax','MPI all reduce failed')
    mpiMaxloc=int(arrayOut(2,:))
#else
    mpiMaxloc=0
    call Galacticus_Error_Report('mpiMaxloc','code was not compiled for MPI')
#endif
    return
  end function mpiMaxloc

  function mpiMinvalArray(self,array,mask)
    !% Find the minimum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                    :: self
    double precision           , intent(in   ), dimension( :          )           :: array
    logical                    , intent(in   ), dimension(0:          ), optional :: mask
    double precision                          , dimension(size(array) )           :: mpiMinvalArray
#ifdef USEMPI
    double precision                          , dimension(size(array) )           :: maskedArray
    integer                                                                       :: iError

    ! Find the minimum over all processes.
    maskedArray=array
    if (present(mask)) then
       if (.not.mask(self%rank())) maskedArray=-HUGE(1.0d0)
    end if
    call MPI_AllReduce(maskedArray,mpiMinvalArray,size(array),MPI_Double_Precision,MPI_Min,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiMinvalArray','MPI all reduce failed')
#else
    mpiMinvalArray=0.0d0
    call Galacticus_Error_Report('mpiMinvalArray','code was not compiled for MPI')
#endif
    return
  end function mpiMinvalArray

  double precision function mpiMinvalScalar(self,scalar,mask)
    !% Find the minimum values of a scalar over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                         :: self
    double precision           , intent(in   )                         :: scalar
    logical                    , intent(in   ), dimension(:), optional :: mask
#ifdef USEMPI
    double precision                          , dimension(1)           :: array

    array=self%minval([scalar],mask)
    mpiMinvalScalar=array(1)
#else
    mpiMinvalScalar=0.0d0
    call Galacticus_Error_Report('mpiMinvalScalar','code was not compiled for MPI')
#endif
    return
  end function mpiMinvalScalar

  function mpiMinloc(self,array,mask)
    !% Find the rank of the process having minimum values of an array over all processes, returning it to all processes.
    implicit none
    class           (mpiObject), intent(in   )                                      :: self
    double precision           , intent(in   ), dimension( :            )           :: array
    logical                    , intent(in   ), dimension(0:            ), optional :: mask
    integer                                   , dimension(   size(array))           :: mpiMinloc
#ifdef USEMPI
    double precision                          , dimension(2 ,size(array))           :: arrayIn  , arrayOut
    integer                                                                         :: iError

    ! Find the minimum over all processes.
    arrayIn(1,:)=array
    if (present(mask)) then
       if (.not.mask(self%rank())) arrayIn(1,:)=-HUGE(1.0d0)
    end if
    arrayIn(2,:)=self%rank()
    call MPI_AllReduce(arrayIn,arrayOut,size(array),MPI_2Double_Precision,MPI_MinLoc,MPI_Comm_World,iError)
    if (iError /= 0) call Galacticus_Error_Report('mpiMin','MPI all reduce failed')
    mpiMinloc=int(arrayOut(2,:))
#else
    mpiMinloc=0
    call Galacticus_Error_Report('mpiMinloc','code was not compiled for MPI')
#endif
    return
  end function mpiMinloc

  function mpiGatherScalar(self,scalar)
    !% Gather a scalar from all processes, returning it as a 1-D array.
    implicit none
    class           (mpiObject), intent(in   )                :: self
    double precision           , intent(in   )                :: scalar
    double precision           , dimension(  self%countValue) :: mpiGatherScalar
#ifdef USEMPI
    double precision           , dimension(1,self%countValue) :: array

    array=self%requestData(self%allRanks,[scalar])
    mpiGatherScalar=array(1,:)
#else
    mpiGatherScalar=0.0d0
    call Galacticus_Error_Report('mpiGatherScalar','code was not compiled for MPI')
#endif
    return
  end function mpiGatherScalar
  
  function mpiGather1D(self,array)
    !% Gather a 1-D array from all processes, returning it as a 2-D array.
    implicit none
    class           (mpiObject), intent(in   )                                         :: self
    double precision           , intent(in   ), dimension(          :                ) :: array
    double precision           ,                dimension(size(array),self%countValue) :: mpiGather1D

#ifdef USEMPI
    mpiGather1D=self%requestData(self%allRanks,array)
#else
    mpiGather1D=0.0d0
    call Galacticus_Error_Report('mpiGather1D','code was not compiled for MPI')
#endif
    return
  end function mpiGather1D

  function mpiGather2D(self,array)
    !% Gather a 1-D array from all processes, returning it as a 2-D array.
    implicit none
    class           (mpiObject), intent(in   )                                                                 :: self
    double precision           , intent(in   ), dimension(                :,                :                ) :: array
    double precision           ,                dimension(size(array,dim=1),size(array,dim=2),self%countValue) :: mpiGather2D

#ifdef USEMPI
    mpiGather2D=self%requestData(self%allRanks,array)
#else
    mpiGather2D=0.0d0
    call Galacticus_Error_Report('mpiGather2D','code was not compiled for MPI')
#endif
    return
  end function mpiGather2D

  function mpiGatherInt1D(self,array)
    !% Gather an integer 1-D array from all processes, returning it as a 2-D array.
    implicit none
    class  (mpiObject), intent(in   )                                         :: self
    integer           , intent(in   ), dimension(          :                ) :: array
    integer           ,                dimension(size(array),self%countValue) :: mpiGatherInt1D

#ifdef USEMPI
    mpiGatherInt1D=self%requestData(self%allRanks,array)
#else
    mpiGatherInt1D=0
    call Galacticus_Error_Report('mpiGatherInt1D','code was not compiled for MPI')
#endif
    return
  end function mpiGatherInt1D

end module MPI_Utilities
