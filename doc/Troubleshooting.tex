\chapter{Troubleshooting}

This chapter contains guidance on solving various problems that you might encounter when running \glc.

\section{Low CPU utilization with large numbers of output redshifts}\index{CPU utilization!low}

If a \glc\ model is failing to make use of the majority of the available CPU cycles, and you are running a model with a large number of output redshifts the problem may be that I/O to disk is limiting the rate at which merger trees can be processed. I/O occurs through the HDF5 library which provides caching functionality. Therefore, this problem can often be mitigated by expanding the size of the HDF5 library's cache. \glc\ allows you to do this using a set of input parameters:
\begin{description}
\item[{\tt [hdf5CacheElementsCount]}] HDF5 limits the number of objects that it will store in its cache. Increasing this number will allow more data to be cached and potentially make disk I/O more efficient. We have had good results by setting this number to some factor (e.g. 2) times the product of the number of output redshifts and the number of properties being output in each snapshot.

\item[{\tt [hdf5CacheSizeBytes]}] HDF5 also limits the size of the cache in bytes. We have had good results setting this to a factor of a few above the product of {\tt [hdf5CacheElementsCount]}, the chunk size (see below) and the size of each output property (8 bytes).

\item[{\tt [hdf5SieveBufferSize]}] HDF5 uses a sieve buffer to speed reading/writing of partial datasets. Increasing the buffer size (specified here in bytes) may improve I/O performance. We have had good results using a value of 512Kb.

\item[{\tt [hdf5UseLatestFormat]}] Normally, HDF5 selects an internal file format to used based on maximum portability. If you set this option to {\tt true} HDF5 will instead use the latest format that it supports---typically allowing it to employ optimizations unavailable to older versions of HDF5. Note that this can make the output file unreadable by older versions of the HDF5 library.
\end{description}
Additionally, you can ensure that compression is switched off in the HDF5 output by setting {\tt [hdf5CompressionLevel]}$=-1$. Finally, adjusting the HDF5 chunk size via the {\tt [hdf5ChunkSize]} parameter may make for more efficient I/O. HDF5 datasets are read/written in chunks of this size. Increasing the size may improve I/O performance. 
